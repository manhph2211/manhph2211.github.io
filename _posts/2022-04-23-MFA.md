---
layout: post
title: Paper Review - MFA
subtitle: Bài 6 TTS
cover-img: /assets/img/path.jpg
# thumbnail-img: /assets/img/thumb.png
share-img: /assets/img/path.jpg
tags: [Speech Synthesis, TTS]
---

# Tổng quan:

Well, đây có vẻ không phải lần đầu tiên chúng ta thấy cái này đúng không - MFA, một open-source command line utility. Thật vậy, chúng ta đã viết về FastSpeech 2 và nó chính xác là đã sử dựng MFA để tạo nhãn cho nhánh duration predictor. Tuy nhiên ở bài đó thì mình chưa viết chi tiết về MFA. Trong quá trình triển khai lấy alignment cho task thử nghiệm mô hình PortaSpeech, mình cũng có dùng MFA. Như vậy, sẽ càng hợp lý để mình viết docs này!
Thực ra trước đây người ta đã cố gắng build những toolkit kiểu này, có thể kể đến như Prosodylab-Aligner và FAVE. MFA được phát triển và is built on top of Kaldi, Kaldi’s more permissive license, so no compilation from source is required by the user. MFA’s use of Kaldi is highly parallel, which mitigates run time when using larger corpora and more computationally-intensive training.
The ASR pipeline that MFA implements uses a standard GMM/HMM architecture, adapted from existing Kaldi recipes. To train a model, monophone GMMs are first iteratively trained and used to generate a basic alignment. Triphone GMMs are then trained to take surrounding phonetic context into account, along with clustering of triphones to combat sparsity. The triphone models are used to generate alignments, which are then used for learning acoustic feature transforms on a per-speaker basis, in order to make the models more applicable to speakers in other datasets
MFA uses mel-frequency cepstral coefficients (MFCCs) as acoustic features. Thirteen MFCCs are calculated with a 25 ms window size and 10 ms frame shift. The feature calculation has a frequency ceiling of 8 kHz, allowing for acoustic models to be built and used regardless of sampling rate (i.e., models trained on 16 kHz sampled files can be applied to 44.1 kHz sampled files without manual resampling). Delta and delta-delta features from surrounding MFCC frames are also included, giving 39 features per frame. Following MFCC generation, CMVN is applied to the features on a per-speaker basis to increase robustness to speaker variability. In the final round of training, feature transforms for each speaker are estimated using feature space Maximum Likelihood Linear Regression (fMLLR). Speaker adaptation is also done when aligning using pre-trained models, but can be disabled for faster alignment. During training, MFA does 40 iterations of monophone GMM training, with realignment done during 20 of the iterations. Following monophone training, 35 iterations of triphone training are done, with 15 iterations that perform realignment. Speaker-adapted triphone training includes another 35 iterations with 15 realignment iterations, as well as 5 iterations that include fMLLR estimation. 
MFA ships with a pre-trained model for English that has been trained on the LibriSpeech corpus (∼1000 hours of audiobooks), and pre-trained acoustic models (mostly from GlobalPhone corpora) and grapheme-phoneme models for generating pronunciation dictionaries are publicly available in the online documentation for 20+ languages. A key feature of MFA is the trainability of acoustic models on new data, as in the Prosodylab-Aligner. Thus, a user can align their dataset either using pre-trained models, or by training from scratch on the dataset. Alignment can be significantly better when using acoustic models trained from scratch—especially when the dataset to be aligned is sufficiently large and varied. 
The TextGrid format allows for the user to specify transcriptions for multiple speakers in the same file. The output of alignment is then a TextGrid for each input file, with separate word and phone tiers for each speaker. 
Instead of requiring every word in the transcripts to be in the pronunciation dictionary, MFA includes an explicit model for unknown words as having a unique phone, which allows them to be modeled while maintaining alignment of surrounding words. The unknown word’s phone is constructed similarly to the silence phone, and can match any amount of vocal noise or speech (e.g. words of different lengths). Before performing alignment, MFA prompts the user if unknown words are found, including their location, to deal with simple typos for existing words. 

